{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ТР 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Partie 1: Presentation des bibliotheque Cles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\xpristo\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pygame in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (2.2.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --user gymnasium pygame numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\xpristo\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pygame in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (2.2.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gymnasium pygame numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01274022,  0.03148451, -0.02066742, -0.01244362], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Partie 2: Ecercices Pratiques avec OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Découverte et Exploration d'un Environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions: Discrete(2)\n",
      "Espace d'observations: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action: 1, Observation: [ 0.01336991  0.22689667 -0.02091629 -0.31157506], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.01790784  0.03207886 -0.02714779 -0.02556119], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01854942  0.2275794  -0.02765902 -0.32668442], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02310101  0.03286193 -0.03419271 -0.04285065], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02375825 -0.16175345 -0.03504972  0.23885109], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02052318  0.03385123 -0.0302727  -0.06467824], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.0212002  -0.1608239  -0.03156626  0.21830186], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01798372  0.03473473 -0.02720023 -0.0841689 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.01867842 -0.15998697 -0.0288836   0.19980973], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01547868  0.03553593 -0.02488741 -0.10184289], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.0161894   0.23100555 -0.02692427 -0.40227267], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02080951  0.03627562 -0.03496972 -0.11819841], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02153502 -0.1583283  -0.03733369  0.16325   ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01836845  0.03730766 -0.03406869 -0.14097302], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01911461  0.23290057 -0.03688815 -0.4442064 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02377262  0.4285245  -0.04577228 -0.7482853 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03234311  0.23406287 -0.06073798 -0.4703507 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03702437  0.03984911 -0.070145   -0.19741327], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03782135 -0.15420307 -0.07409326  0.07234348], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.03473729  0.04189859 -0.07264639 -0.24276546], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03557526 -0.15211456 -0.0775017   0.02614741], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03253297 -0.34604445 -0.07697875  0.29340637], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02561208 -0.14991419 -0.07111063 -0.02252719], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.0226138   0.04615169 -0.07156117 -0.33677167], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02353683  0.24221519 -0.0782966  -0.6511352 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02838113  0.4383353  -0.09131931 -0.9674098 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.03714784  0.63455695 -0.1106675  -1.2873265 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.04983898  0.4410034  -0.13641404 -1.0312421 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.05865905  0.24793382 -0.15703888 -0.78430957], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.06361772  0.05527812 -0.17272507 -0.544862  ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.06472328  0.2523525  -0.1836223  -0.8866039 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.06977034  0.06013406 -0.20135438 -0.6568055 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.07097302 -0.13170032 -0.2144905  -0.43366647], Reward: 1.0\n",
      "Action: 1, Observation: [-0.01309457  0.21360615 -0.02487421 -0.34352532], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00882244  0.40907297 -0.03174472 -0.6439471 ], Reward: 1.0\n",
      "Action: 1, Observation: [-6.409838e-04  6.046226e-01 -4.462366e-02 -9.464552e-01], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01145147  0.8003162  -0.06355277 -1.2528183 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02745779  0.60606337 -0.08860913 -0.98069924], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.03957906  0.80225396 -0.10822312 -1.2998476 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.05562414  0.99857026 -0.13422006 -1.6243548 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.07559554  0.80525905 -0.16670717 -1.3763382 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.09170073  0.6125654  -0.19423392 -1.1400901 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.10395204  0.8096215  -0.21703574 -1.4868624 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01591305 -0.18917894  0.04016573  0.34008732], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01969663 -0.38484868  0.04696747  0.6451608 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.0273936  -0.58059263  0.05987069  0.9522562 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03900545 -0.7764669   0.07891581  1.2631327 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.05453479 -0.9725041   0.10417847  1.5794505 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.07398488 -0.7787656   0.13576747  1.3209914 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08956019 -0.97531694  0.16218731  1.6528984 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.10906652 -1.1719187   0.19524527  1.9914054 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.1325049  -0.97930616  0.23507337  1.7650139 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.00224561 -0.16086811  0.01775569  0.3387984 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00546297  0.03399675  0.02453166  0.0517671 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.00478304 -0.16146821  0.025567    0.35208791], Reward: 1.0\n",
      "Action: 0, Observation: [-0.0080124  -0.35694423  0.03260876  0.652722  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01515129 -0.5525047   0.0456632   0.9554921 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02620138 -0.7482102   0.06477305  1.2621646 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04116559 -0.9440978   0.09001634  1.5744098 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.06004754 -1.1401705   0.12150453  1.893757  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08285095 -1.3363829   0.15937968  2.2215402 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.10957861 -1.5326262   0.20381047  2.558834  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.14023113 -1.72871     0.25498715  2.9063785 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.0138228   0.17268968 -0.01371528 -0.270821  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01036901 -0.0222339  -0.0191317   0.0175047 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01081368 -0.21707633 -0.01878161  0.30409047], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01515521 -0.41192564 -0.0126998   0.59079146], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02339372 -0.21662822 -0.00088397  0.29413527], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02772629 -0.41173756  0.00499874  0.58653927], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03596104 -0.21668597  0.01672952  0.2954352 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04029476 -0.41204238  0.02263822  0.593347  ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04853561 -0.2172445   0.03450517  0.3078802 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.0528805  -0.4128407   0.04066277  0.61124235], Reward: 1.0\n",
      "Action: 0, Observation: [-0.06113731 -0.6085067   0.05288761  0.91645026], Reward: 1.0\n",
      "Action: 1, Observation: [-0.07330745 -0.41413823  0.07121662  0.64084697], Reward: 1.0\n",
      "Action: 1, Observation: [-0.08159021 -0.22007757  0.08403356  0.37141383], Reward: 1.0\n",
      "Action: 1, Observation: [-0.08599176 -0.02624375  0.09146184  0.10636586], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08651663 -0.22254926  0.09358916  0.4264457 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.09096762 -0.41886356  0.10211807  0.7471035 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.09934489 -0.22528766  0.11706014  0.48822287], Reward: 1.0\n",
      "Action: 1, Observation: [-0.10385065 -0.03199499  0.1268246   0.23460525], Reward: 1.0\n",
      "Action: 1, Observation: [-0.10449055  0.1611083   0.1315167  -0.01553676], Reward: 1.0\n",
      "Action: 1, Observation: [-0.10126838  0.35412294  0.13120596 -0.2640039 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.09418592  0.54715186  0.12592588 -0.51259464], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08324289  0.35050213  0.115674   -0.18303129], Reward: 1.0\n",
      "Action: 1, Observation: [-0.07623284  0.5437953   0.11201337 -0.43710104], Reward: 1.0\n",
      "Action: 0, Observation: [-0.06535693  0.3472808   0.10327135 -0.11131353], Reward: 1.0\n",
      "Action: 1, Observation: [-0.05841132  0.540783    0.10104508 -0.36971256], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04759566  0.34438142  0.09365083 -0.04695617], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04070803  0.5380444   0.0927117  -0.3086846 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02994714  0.7317315   0.08653801 -0.5707492 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.01531251  0.9255401   0.07512303 -0.83496386], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.00319829  1.1195598   0.05842375 -1.1031072 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02558948  0.92372     0.0363616  -0.79268193], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.04406388  0.72811824  0.02050797 -0.48878527], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.05862625  0.53271306  0.01073226 -0.1897102 ], Reward: 1.0\n",
      "Action: 0, Observation: [0.06928051 0.33743924 0.00693806 0.10633891], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.07602929  0.53246105  0.00906484 -0.18414705], Reward: 1.0\n",
      "Action: 0, Observation: [0.08667852 0.3372106  0.0053819  0.11138166], Reward: 1.0\n",
      "Action: 0, Observation: [0.09342273 0.14201194 0.00760953 0.40575767], Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espace d'actions: {env.action_space}\")\n",
    "print(f\"Espace d'observations: {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Observation: {observation}, Reward: {reward}\")\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Manipulation des Observations et Récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 **Essai 1** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.01246711 -0.23111808  0.02237324  0.31197724]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 2** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.00784475 -0.42655152  0.02861279  0.6116312 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 3** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [-6.8627793e-04 -6.2206143e-01  4.0845413e-02  9.1318703e-01]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 4** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [-0.01312751 -0.42751512  0.05910916  0.6336162 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 5** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [-0.02167781 -0.23326539  0.07178148  0.36011842]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 6** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [-0.02634312 -0.0392333   0.07898384  0.09090544]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 7** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [-0.02712778  0.15467292  0.08080196 -0.17584999]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 8** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [-0.02403433 -0.04150687  0.07728495  0.1411895 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 9** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [-0.02486446  0.15242803  0.08010875 -0.12614533]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 10** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [-0.0218159   0.3463164   0.07758584 -0.3925184 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 11** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [-0.01488957  0.5402565   0.06973547 -0.6597661 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 12** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [-0.00408444  0.34423694  0.05654015 -0.34596574]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 13** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.0028003   0.5385109   0.04962083 -0.62029713]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 14** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.01357051  0.73290604  0.03721489 -0.89694834]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 15** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.02822863  0.5372999   0.01927592 -0.5928036 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 16** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.03897463  0.7321468   0.00741985 -0.87935287]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 17** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.05361757  0.9271671  -0.01016721 -1.169694  ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 18** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.07216091  1.1224198  -0.03356108 -1.465547  ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 19** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.09460931  0.92772454 -0.06287202 -1.1835337 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 20** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.1131638  1.1236033 -0.0865427 -1.4952435]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 21** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.13563587  1.3196646  -0.11644757 -1.8136466 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 22** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.16202916  1.5158749  -0.1527205  -2.140127  ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 23** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.19234666  1.712141   -0.19552304 -2.475825  ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 24** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.22658947  1.9082925  -0.24503954 -2.8215644 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Oui\n",
      "========================================\n",
      "🔄 Réinitialisation de l'environnement...\n",
      "\n",
      "\n",
      "🔹 **Essai 25** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.0181154   0.163647    0.03056779 -0.29060265]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 26** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.02138834 -0.03189718  0.02475573  0.01156213]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 27** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.02075039  0.16286115  0.02498698 -0.2732084 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 28** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.02400761  0.35761783  0.01952281 -0.5579068 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 29** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.03115997  0.16222733  0.00836467 -0.2591376 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 30** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.03440452  0.35722888  0.00318192 -0.5491705 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 31** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.0415491   0.552306   -0.00780149 -0.8408492 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 32** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.05259522  0.3572914  -0.02461847 -0.55062985]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 33** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.05974104  0.55275035 -0.03563107 -0.8509665 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 34** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.07079605  0.35813186 -0.0526504  -0.5696974 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 35** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.07795869  0.1637863  -0.06404435 -0.2940551 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 36** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.08123441 -0.03036687 -0.06992545 -0.02223856]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 37** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.08062708 -0.22441997 -0.07037022  0.24758843]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 38** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.07613868 -0.41847005 -0.06541845  0.51727134]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 39** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.06776927 -0.22249092 -0.05507303  0.20471217]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 40** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.06331946 -0.4167838  -0.05097878  0.47952637]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 41** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.05498378 -0.22098066 -0.04138825  0.17122191]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 42** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.05056417 -0.41548654 -0.03796382  0.4505662 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 43** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.04225444 -0.21984878 -0.02895249  0.14616235]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 44** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.03785746 -0.02432444 -0.02602925 -0.15551211]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 45** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.03737097  0.17116034 -0.02913949 -0.45629168]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 46** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.04079418 -0.02353777 -0.03826532 -0.1729343 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 47** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.04032343  0.17211036 -0.04172401 -0.4774389 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 48** :\n",
      "  ➡️ Action choisie      : 1\n",
      "  🔄 Nouvelle observation : [ 0.04376563  0.36779583 -0.05127279 -0.7829749 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 49** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.05112155  0.1734146  -0.06693228 -0.5068539 ]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n",
      "\n",
      "🔹 **Essai 50** :\n",
      "  ➡️ Action choisie      : 0\n",
      "  🔄 Nouvelle observation : [ 0.05458984 -0.02070353 -0.07706936 -0.23599206]\n",
      "  🏆 Récompense obtenue  : 1.0\n",
      "  ✔️ Épisode terminé ?   : Non\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "#import gym\n",
    "\n",
    "# Création de l'environnement\n",
    "env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Boucle d'essais\n",
    "for essai in range(50):\n",
    "    action = env.action_space.sample()  # Sélection d'une action aléatoire\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # Affichage formaté\n",
    "    print(f\"\\n🔹 **Essai {essai + 1}** :\")\n",
    "    print(f\"  ➡️ Action choisie      : {action}\")\n",
    "    print(f\"  🔄 Nouvelle observation : {observation}\")\n",
    "    print(f\"  🏆 Récompense obtenue  : {reward}\")\n",
    "    print(f\"  ✔️ Épisode terminé ?   : {'Oui' if done else 'Non'}\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Réinitialisation si l'épisode est terminé\n",
    "    if done:\n",
    "        print(\"🔄 Réinitialisation de l'environnement...\\n\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Fermeture de l'environnement\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Contrôle Manuel de l'Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrôle manuel : tape 0 (gauche) ou 1 (droite)\n",
      "\n",
      "Observation actuelle : [ 0.04990437  0.03518724  0.04455953 -0.02307701]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [ 0.05060811 -0.16054447  0.04409799  0.28332528]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [ 0.05060811 -0.16054447  0.04409799  0.28332528]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 1\n",
      "Nouvelle observation : [0.04739723 0.03392167 0.04976449 0.00487027]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [0.04739723 0.03392167 0.04976449 0.00487027]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [ 0.04807566 -0.16187735  0.0498619   0.3128299 ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [ 0.04807566 -0.16187735  0.0498619   0.3128299 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [ 0.04483811 -0.35767287  0.0561185   0.6208115 ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [ 0.04483811 -0.35767287  0.0561185   0.6208115 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [ 0.03768466 -0.55353177  0.06853472  0.9306275 ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [ 0.03768466 -0.55353177  0.06853472  0.9306275 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [ 0.02661402 -0.74950844  0.08714727  1.2440358 ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [ 0.02661402 -0.74950844  0.08714727  1.2440358 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 1\n",
      "Nouvelle observation : [ 0.01162385 -0.555606    0.112028    0.9798749 ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [ 0.01162385 -0.555606    0.112028    0.9798749 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [ 5.1173044e-04 -7.5203687e-01  1.3162549e-01  1.3055415e+00]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [ 5.1173044e-04 -7.5203687e-01  1.3162549e-01  1.3055415e+00]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [-0.01452901 -0.94855887  0.15773632  1.6363609 ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [-0.01452901 -0.94855887  0.15773632  1.6363609 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [-0.03350018 -1.1451404   0.19046354  1.9737548 ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "Observation actuelle : [-0.03350018 -1.1451404   0.19046354  1.9737548 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action choisie : 0\n",
      "Nouvelle observation : [-0.05640299 -1.3416933   0.22993864  2.318911  ]\n",
      "Récompense reçue : 1.0\n",
      "\n",
      "L'épisode est terminé.\n",
      "Nombre total d'actions : 11\n"
     ]
    }
   ],
   "source": [
    "#import gym\n",
    "\n",
    "# Création de l’environnement\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "step_count = 0  # Nombre total d'actions effectuées\n",
    "\n",
    "print(\"Contrôle manuel : tape 0 (gauche) ou 1 (droite)\")\n",
    "\n",
    "while not done:\n",
    "    # Affichage de l'observation actuelle\n",
    "    print(f\"\\nObservation actuelle : {observation}\")\n",
    "\n",
    "    # Demande de l'action à l'utilisateur\n",
    "    user_input = input(\"Tape ton action (0=gauche, 1=droite) : \")\n",
    "\n",
    "    # Vérification de la validité de l'entrée\n",
    "    if user_input not in {\"0\", \"1\"}:\n",
    "        print(\"Entrée invalide. Merci de taper uniquement 0 ou 1.\")\n",
    "        continue\n",
    "\n",
    "    action = int(user_input)\n",
    "\n",
    "    # Exécution de l'action et mise à jour de l'état\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    step_count += 1\n",
    "\n",
    "    # Affichage des informations après l'action\n",
    "    print(f\"Action choisie : {action}\")\n",
    "    print(f\"Nouvelle observation : {observation}\")\n",
    "    print(f\"Récompense reçue : {reward}\")\n",
    "\n",
    "# Fin de l'épisode\n",
    "print(\"\\nL'épisode est terminé.\")\n",
    "print(f\"Nombre total d'actions : {step_count}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: Évaluation des Performances d'une Politique Aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 1 terminé après 27 étapes.\n",
      "Épisode 2 terminé après 40 étapes.\n",
      "Épisode 3 terminé après 23 étapes.\n",
      "Épisode 4 terminé après 23 étapes.\n",
      "Épisode 5 terminé après 42 étapes.\n",
      "Épisode 6 terminé après 20 étapes.\n",
      "Épisode 7 terminé après 18 étapes.\n",
      "Épisode 8 terminé après 67 étapes.\n",
      "Épisode 9 terminé après 12 étapes.\n",
      "Épisode 10 terminé après 19 étapes.\n",
      "\n",
      "Résultats sur 10 épisodes aléatoires :\n",
      "Durées des épisodes : [27, 40, 23, 23, 42, 20, 18, 67, 12, 19]\n",
      "Durée moyenne : 29.10 étapes\n"
     ]
    }
   ],
   "source": [
    "#import gym\n",
    "\n",
    "# Création de l'environnement\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "nombre_episodes = 10\n",
    "resultats = []\n",
    "\n",
    "for episode in range(1, nombre_episodes + 1):\n",
    "    observation, info = env.reset()\n",
    "    termine = False\n",
    "    compteur_etapes = 0\n",
    "\n",
    "    while not termine:\n",
    "        action = env.action_space.sample()  # Choix d'une action aléatoire\n",
    "        observation, recompense, termine, tronque, info = env.step(action)\n",
    "        termine = termine or tronque\n",
    "        compteur_etapes += 1\n",
    "\n",
    "    resultats.append(compteur_etapes)\n",
    "    print(f\"Épisode {episode} terminé après {compteur_etapes} étapes.\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Calcul et affichage des statistiques\n",
    "moyenne_duree = sum(resultats) / nombre_episodes\n",
    "print(\"\\nRésultats sur 10 épisodes aléatoires :\")\n",
    "print(f\"Durées des épisodes : {resultats}\")\n",
    "print(f\"Durée moyenne : {moyenne_duree:.2f} étapes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
