{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING II - ТР 1\n",
    "\n",
    "\n",
    "Objectif:\n",
    "\n",
    "L'objectif de ce TP est de se familiariser avec les outils essentiels du Reinforcement Learning(RL), notamment OpenAI Gym. Les étudiants vont explorer comment interagir avec un environnement RL et exécuter des actions avant d'implémenter un algorithme d'apprentissage dans les séances suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Partie 1: Presentation des bibliotheque Cles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. OpenAl Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OpenAl Gym est une bibliothèque permettant de simuler des environnements interactifs pour tester des algorithmes de RL.\n",
    "- Un environnement Gym est défini par un ensemble d'états, d'actions, de récompenses et d'un critère de fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation de Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (1.1.0)\n",
      "Requirement already satisfied: pygame in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xpristo\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D24F7170>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gymnasium/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D64CC3B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gymnasium/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D649C350>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gymnasium/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D646CC50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gymnasium/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D646DC40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gymnasium/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D74D0DA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pygame/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D6101490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pygame/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D646CBF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pygame/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D649FE60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pygame/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D61E7470>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pygame/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D74D1400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D74D16A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D74D1A00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D74D1C40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A6D74D1EE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gymnasium pygame numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03468746,  0.00549068, -0.04994776,  0.02527633], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Partie 2: Ecercices Pratiques avec OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Découverte et Exploration d'un Environnement Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif: Comprendre la structure d'un environnement Gym en explorant ses propriétés et ses actions possibles.\n",
    "\n",
    "\n",
    "✔Instructions :\n",
    "1. Afficher l'espace d'actions et l'espace d'observations.\n",
    "2. Exécuter une boucle de simulation avec des actions aléatoires pendant 100 itérations.\n",
    "3. Observer les valeurs des observations retournées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions: Discrete(2)\n",
      "Espace d'observations: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action: 1, Observation: [ 0.03479727  0.20129202 -0.04944223 -0.28273794], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.03882312  0.39708304 -0.05509699 -0.59059566], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.04676478  0.20277403 -0.0669089  -0.31576476], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.05082026  0.39878207 -0.0732242  -0.62877554], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.0587959   0.5948454  -0.08579971 -0.9435911 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.07069281  0.40097767 -0.10467153 -0.67905337], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.07871236  0.2074535  -0.1182526  -0.42107314], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.08286143  0.40403497 -0.12667406 -0.7485714 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.09094213  0.21086688 -0.14164549 -0.4982835 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.09515947  0.407672   -0.15161116 -0.8320393 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.10331291  0.21491091 -0.16825195 -0.59061754], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.10761113  0.02249449 -0.1800643  -0.35530162], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.10806102  0.21965858 -0.18717033 -0.6989195 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.11245419  0.0275567  -0.20114872 -0.47050667], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.11300533  0.22486678 -0.21055885 -0.8192442 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.04233053  0.1562734  -0.03642515 -0.27675915], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.045456    0.35189557 -0.04196034 -0.5807044 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.05249391  0.15738593 -0.05357442 -0.30152947], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.05564163  0.3532289  -0.05960501 -0.6106158 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.06270621  0.15898852 -0.07181733 -0.3372861 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.06588598 -0.03504187 -0.07856305 -0.06808694], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.06518514  0.1611134  -0.07992479 -0.38448608], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.06840741  0.3572737  -0.08761451 -0.7012601 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.07555289  0.55349386 -0.10163972 -1.0201871 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.08662276  0.74981254 -0.12204346 -1.342976  ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.10161901  0.556419   -0.14890298 -1.0908352 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.11274739  0.36353955 -0.17071968 -0.84833336], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.12001818  0.56052727 -0.18768635 -1.1894675 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.13122873  0.75751877 -0.2114757  -1.5346221 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.01897216 -0.23115082  0.03755423  0.32562533], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01434914 -0.03658313  0.04406673  0.04501792], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01361748  0.15788013  0.04496709 -0.23344237], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01677508  0.3523317   0.04029824 -0.511609  ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02382171  0.15666595  0.03006606 -0.20650421], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02695503 -0.03887274  0.02593598  0.09550937], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02617758 -0.23435664  0.02784617  0.39626092], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02149044 -0.42986238  0.03577138  0.69759154], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.0128932  -0.23525424  0.04972322  0.41638064], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.00818811 -0.43104434  0.05805083  0.7243154 ], Reward: 1.0\n",
      "Action: 1, Observation: [-4.327747e-04 -2.367712e-01  7.253714e-02  4.504539e-01], Reward: 1.0\n",
      "Action: 0, Observation: [-0.0051682  -0.43284008  0.08154622  0.76509106], Reward: 1.0\n",
      "Action: 0, Observation: [-0.013825   -0.62898463  0.09684803  1.0822783 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02640469 -0.82524204  0.1184936   1.4037137 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04290953 -0.6317742   0.14656788  1.1503025 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.05554502 -0.8284727   0.16957392  1.4851235 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.07211447 -0.63577473  0.19927639  1.2498387 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08482996 -0.83281285  0.22427318  1.5977471 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00435531  0.22523749 -0.03500666 -0.3009167 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.00014944  0.03063153 -0.04102499 -0.01947645], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.00076207 -0.1638788  -0.04141452  0.2599856 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00251551  0.03180914 -0.03621481 -0.04546703], Reward: 1.0\n",
      "Action: 0, Observation: [-0.00187933 -0.1627753  -0.03712415  0.23557343], Reward: 1.0\n",
      "Action: 0, Observation: [-0.00513483 -0.35734773 -0.03241268  0.516319  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01228179 -0.5519986  -0.0220863   0.7986143 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02332176 -0.35658073 -0.00611402  0.4990661 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03045337 -0.16137312  0.00386731  0.20446266], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03368083  0.03369331  0.00795656 -0.08699781], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03300697 -0.16154179  0.0062166   0.20818476], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03623781  0.03349072  0.0103803  -0.08253068], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03556799 -0.16177848  0.00872969  0.2134091 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03880356  0.03321759  0.01299787 -0.07650735], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03813921  0.22815081  0.01146772 -0.36506122], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03357619  0.42310792  0.0041665  -0.65410626], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02511403  0.2279282  -0.00891563 -0.36011428], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02055547  0.03293413 -0.01611791 -0.07025591], Reward: 1.0\n",
      "Action: 1, Observation: [-0.01989679  0.2282834  -0.01752303 -0.3679802 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01533112  0.03341477 -0.02488264 -0.08087366], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01466282 -0.16134182 -0.02650011  0.20385604], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01788966 -0.35607496 -0.02242299  0.4880631 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02501116 -0.5508735  -0.01266173  0.77359563], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03602863 -0.35557964  0.00281019  0.4769559 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04314022 -0.1604975   0.0123493   0.18516   ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04635017  0.03444561  0.0160525  -0.10360175], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04566126 -0.16090268  0.01398047  0.19410215], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04887931  0.03401653  0.01786251 -0.09413796], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04819898  0.22887796  0.01597975 -0.3811322 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04362142  0.4237694   0.00835711 -0.6687342 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.03514604  0.6187742  -0.00501758 -0.95877415], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02277055  0.81396323 -0.02419306 -1.2530292 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00649129  1.0093865  -0.04925364 -1.5531905 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01369644  1.2050632  -0.08031745 -1.8608241 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03779771  1.0109086  -0.11753394 -1.5941191 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.05801588  0.81736076 -0.14941631 -1.3402758 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.07436309  1.014014   -0.17622183 -1.6757343 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.09464338  1.2106887  -0.20973651 -2.0177188 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.00290511  0.21055588 -0.02841564 -0.25838566], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.00711622  0.40607175 -0.03358335 -0.55989414], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01523766  0.60164857 -0.04478123 -0.8629656 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02727063  0.40716398 -0.06204055 -0.5846925 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.03541391  0.6030976  -0.0737344  -0.8962557 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.04747586  0.40904862 -0.09165951 -0.62763137], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.05565683  0.60532224 -0.10421214 -0.9477161 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.06776328  0.80168134 -0.12316646 -1.2712398 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.0837969   0.6083278  -0.14859125 -1.019526  ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.09596346  0.8050839  -0.16898178 -1.3549331 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.11206514  0.61243695 -0.19608043 -1.1195233 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.12431388  0.42035085 -0.2184709  -0.89419425], Reward: 1.0\n",
      "Action: 1, Observation: [-0.01174896  0.21692546  0.04839981 -0.26055008], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00741045  0.41132426  0.04318881 -0.5375829 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 8.1603782e-04  6.0581326e-01  3.2437153e-02 -8.1635070e-01], Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espace d'actions: {env.action_space}\")\n",
    "print(f\"Espace d'observations: {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Observation: {observation}, Reward: {reward}\")\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Manipulation des Observations et Récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif: Comprendre comment récupérer les observations et les récompenses lors de l'interaction avec l'environnement.\n",
    "\n",
    "\n",
    "✔Instructions:\n",
    "1. Prendre une action et récupérer les valeurs retournées (observation, reward, done).\n",
    "2. Afficher ces valeurs et analyser leur signification.\n",
    "3. Faire plusieurs essais et noter les variations des observations et des récompenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai 1\n",
      "Action choisie : 0\n",
      "Observation : [ 0.00752668 -0.21072377 -0.01721429  0.24016707]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 2\n",
      "Action choisie : 1\n",
      "Observation : [ 0.0033122  -0.01536019 -0.01241094 -0.05789562]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 3\n",
      "Action choisie : 0\n",
      "Observation : [ 0.003005   -0.21030201 -0.01356886  0.23084587]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 4\n",
      "Action choisie : 0\n",
      "Observation : [-0.00120104 -0.40522748 -0.00895194  0.519218  ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 5\n",
      "Action choisie : 1\n",
      "Observation : [-0.00930559 -0.20998064  0.00143242  0.22372767]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 6\n",
      "Action choisie : 0\n",
      "Observation : [-0.01350521 -0.40512303  0.00590697  0.5168621 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 7\n",
      "Action choisie : 0\n",
      "Observation : [-0.02160767 -0.6003277   0.01624422  0.8114006 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 8\n",
      "Action choisie : 0\n",
      "Observation : [-0.03361422 -0.7956683   0.03247223  1.1091485 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 9\n",
      "Action choisie : 0\n",
      "Observation : [-0.04952759 -0.9912016   0.0546552   1.411839  ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 10\n",
      "Action choisie : 1\n",
      "Observation : [-0.06935162 -0.79679817  0.08289198  1.1367298 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 11\n",
      "Action choisie : 0\n",
      "Observation : [-0.08528758 -0.9929007   0.10562658  1.454216  ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 12\n",
      "Action choisie : 0\n",
      "Observation : [-0.1051456 -1.1891491  0.1347109  1.7779444]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 13\n",
      "Action choisie : 1\n",
      "Observation : [-0.12892857 -0.99577683  0.17026979  1.5299993 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 14\n",
      "Action choisie : 1\n",
      "Observation : [-0.14884411 -0.8030678   0.20086977  1.2949336 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 15\n",
      "Action choisie : 1\n",
      "Observation : [-0.16490547 -0.61098284  0.22676845  1.0712588 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : True\n",
      "----------------------------------------\n",
      "L’épisode est terminé. Réinitialisation de l’environnement.\n",
      "\n",
      "Essai 16\n",
      "Action choisie : 0\n",
      "Observation : [ 0.01843521 -0.21656731 -0.0154919   0.24285366]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 17\n",
      "Action choisie : 0\n",
      "Observation : [ 0.01410386 -0.41146457 -0.01063482  0.53061   ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 18\n",
      "Action choisie : 0\n",
      "Observation : [ 5.8745709e-03 -6.0643536e-01 -2.2621285e-05  8.1992304e-01]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 19\n",
      "Action choisie : 0\n",
      "Observation : [-0.00625414 -0.801557    0.01637584  1.1125989 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 20\n",
      "Action choisie : 1\n",
      "Observation : [-0.02228528 -0.60665387  0.03862782  0.82509774]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 21\n",
      "Action choisie : 0\n",
      "Observation : [-0.03441835 -0.8022823   0.05512977  1.1296751 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 22\n",
      "Action choisie : 0\n",
      "Observation : [-0.050464   -0.99808115  0.07772327  1.4391272 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 23\n",
      "Action choisie : 1\n",
      "Observation : [-0.07042562 -0.8039981   0.10650582  1.1717092 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 24\n",
      "Action choisie : 0\n",
      "Observation : [-0.08650558 -1.0003313   0.12994     1.4957937 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 25\n",
      "Action choisie : 1\n",
      "Observation : [-0.10651221 -0.8070066   0.15985587  1.2463464 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 26\n",
      "Action choisie : 0\n",
      "Observation : [-0.12265234 -1.0037762   0.1847828   1.5845354 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 27\n",
      "Action choisie : 0\n",
      "Observation : [-0.14272787 -1.2005522   0.2164735   1.928692  ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : True\n",
      "----------------------------------------\n",
      "L’épisode est terminé. Réinitialisation de l’environnement.\n",
      "\n",
      "Essai 28\n",
      "Action choisie : 0\n",
      "Observation : [-0.04297378 -0.1930581   0.00251125  0.2615516 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 29\n",
      "Action choisie : 0\n",
      "Observation : [-0.04683494 -0.3882158   0.00774229  0.5550256 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 30\n",
      "Action choisie : 0\n",
      "Observation : [-0.05459926 -0.5834456   0.0188428   0.85013765]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 31\n",
      "Action choisie : 0\n",
      "Observation : [-0.06626817 -0.7788194   0.03584555  1.1486857 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 32\n",
      "Action choisie : 0\n",
      "Observation : [-0.08184456 -0.97439045  0.05881926  1.4523902 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 33\n",
      "Action choisie : 0\n",
      "Observation : [-0.10133237 -1.1701835   0.08786707  1.7628548 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 34\n",
      "Action choisie : 1\n",
      "Observation : [-0.12473604 -0.9761585   0.12312417  1.4987398 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 35\n",
      "Action choisie : 1\n",
      "Observation : [-0.14425921 -0.7827289   0.15309897  1.2468989 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 36\n",
      "Action choisie : 0\n",
      "Observation : [-0.1599138  -0.9794463   0.17803694  1.583359  ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 37\n",
      "Action choisie : 1\n",
      "Observation : [-0.17950271 -0.78683287  0.20970413  1.3510727 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : True\n",
      "----------------------------------------\n",
      "L’épisode est terminé. Réinitialisation de l’environnement.\n",
      "\n",
      "Essai 38\n",
      "Action choisie : 0\n",
      "Observation : [-0.00282515 -0.20639874 -0.02011225  0.25891873]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 39\n",
      "Action choisie : 1\n",
      "Observation : [-0.00695312 -0.01099553 -0.01493388 -0.04003942]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 40\n",
      "Action choisie : 1\n",
      "Observation : [-0.00717303  0.18433735 -0.01573467 -0.33739647]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 41\n",
      "Action choisie : 0\n",
      "Observation : [-0.00348629 -0.0105572  -0.0224826  -0.04971664]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 42\n",
      "Action choisie : 1\n",
      "Observation : [-0.00369743  0.1848798  -0.02347693 -0.34940737]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 43\n",
      "Action choisie : 1\n",
      "Observation : [ 1.6571397e-07  3.8032764e-01 -3.0465078e-02 -6.4939994e-01]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 44\n",
      "Action choisie : 1\n",
      "Observation : [ 0.00760672  0.57586044 -0.04345307 -0.95151836]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 45\n",
      "Action choisie : 0\n",
      "Observation : [ 0.01912393  0.38134935 -0.06248344 -0.67279834]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 46\n",
      "Action choisie : 0\n",
      "Observation : [ 0.02675091  0.18714902 -0.07593941 -0.40042448]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 47\n",
      "Action choisie : 1\n",
      "Observation : [ 0.03049389  0.3832614  -0.0839479  -0.71604997]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 48\n",
      "Action choisie : 0\n",
      "Observation : [ 0.03815912  0.18939553 -0.0982689  -0.45092696]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 49\n",
      "Action choisie : 0\n",
      "Observation : [ 0.04194703 -0.00420926 -0.10728744 -0.19076541]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 50\n",
      "Action choisie : 1\n",
      "Observation : [ 0.04186285  0.19227092 -0.11110274 -0.51527315]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 51\n",
      "Action choisie : 1\n",
      "Observation : [ 0.04570827  0.3887677  -0.12140821 -0.8407981 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 52\n",
      "Action choisie : 1\n",
      "Observation : [ 0.05348362  0.5853193  -0.13822417 -1.1690617 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 53\n",
      "Action choisie : 0\n",
      "Observation : [ 0.06519    0.392239  -0.1616054 -0.9227122]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 54\n",
      "Action choisie : 0\n",
      "Observation : [ 0.07303479  0.1996258  -0.18005964 -0.6848624 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 55\n",
      "Action choisie : 1\n",
      "Observation : [ 0.0770273  0.3967301 -0.1937569 -1.0283909]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 56\n",
      "Action choisie : 0\n",
      "Observation : [ 0.08496191  0.20464031 -0.21432471 -0.80225664]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : True\n",
      "----------------------------------------\n",
      "L’épisode est terminé. Réinitialisation de l’environnement.\n",
      "\n",
      "Essai 57\n",
      "Action choisie : 0\n",
      "Observation : [ 0.03787317 -0.19386323  0.01372588  0.28817305]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 58\n",
      "Action choisie : 0\n",
      "Observation : [ 0.03399591 -0.3891782   0.01948934  0.5851532 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 59\n",
      "Action choisie : 1\n",
      "Observation : [ 0.02621235 -0.19433458  0.0311924   0.2986728 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 60\n",
      "Action choisie : 1\n",
      "Observation : [0.02232565 0.00032917 0.03716586 0.01598827]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 61\n",
      "Action choisie : 1\n",
      "Observation : [ 0.02233224  0.19489896  0.03748562 -0.26474062]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 62\n",
      "Action choisie : 0\n",
      "Observation : [ 0.02623022 -0.00073742  0.03219081  0.03952599]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 63\n",
      "Action choisie : 0\n",
      "Observation : [ 0.02621547 -0.19630584  0.03298133  0.3421891 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 64\n",
      "Action choisie : 1\n",
      "Observation : [ 0.02228935 -0.00166827  0.03982511  0.06008628]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 65\n",
      "Action choisie : 1\n",
      "Observation : [ 0.02225599  0.19286071  0.04102684 -0.21977034]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 66\n",
      "Action choisie : 1\n",
      "Observation : [ 0.0261132   0.38737294  0.03663143 -0.49923465]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 67\n",
      "Action choisie : 0\n",
      "Observation : [ 0.03386066  0.1917542   0.02664674 -0.19523616]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 68\n",
      "Action choisie : 1\n",
      "Observation : [ 0.03769574  0.38648504  0.02274201 -0.47939554]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 69\n",
      "Action choisie : 0\n",
      "Observation : [ 0.04542544  0.19104956  0.0131541  -0.17963251]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 70\n",
      "Action choisie : 1\n",
      "Observation : [ 0.04924643  0.3859808   0.00956145 -0.46813694]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 71\n",
      "Action choisie : 1\n",
      "Observation : [ 5.6966051e-02  5.8096641e-01  1.9871493e-04 -7.5779092e-01]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 72\n",
      "Action choisie : 1\n",
      "Observation : [ 0.06858538  0.7760856  -0.0149571  -1.0504113 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 73\n",
      "Action choisie : 1\n",
      "Observation : [ 0.08410709  0.97140276 -0.03596533 -1.3477514 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 74\n",
      "Action choisie : 0\n",
      "Observation : [ 0.10353515  0.77675086 -0.06292035 -1.0665339 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 75\n",
      "Action choisie : 0\n",
      "Observation : [ 0.11907016  0.5825153  -0.08425104 -0.7942435 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 76\n",
      "Action choisie : 0\n",
      "Observation : [ 0.13072047  0.38864458 -0.10013591 -0.52920943]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 77\n",
      "Action choisie : 0\n",
      "Observation : [ 0.13849336  0.19506338 -0.1107201  -0.26968303]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 78\n",
      "Action choisie : 0\n",
      "Observation : [ 0.14239463  0.00168128 -0.11611376 -0.01387132]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 79\n",
      "Action choisie : 0\n",
      "Observation : [ 0.14242825 -0.19160056 -0.11639118  0.24003842]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 80\n",
      "Action choisie : 1\n",
      "Observation : [ 0.13859624  0.00497508 -0.11159042 -0.08697183]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 81\n",
      "Action choisie : 0\n",
      "Observation : [ 0.13869575 -0.18838513 -0.11332985  0.16852495]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 82\n",
      "Action choisie : 0\n",
      "Observation : [ 0.13492805 -0.38171786 -0.10995935  0.42341602]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 83\n",
      "Action choisie : 1\n",
      "Observation : [ 0.12729369 -0.18522395 -0.10149103  0.09819229]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 84\n",
      "Action choisie : 0\n",
      "Observation : [ 0.1235892  -0.37875593 -0.09952719  0.3572093 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 85\n",
      "Action choisie : 0\n",
      "Observation : [ 0.11601409 -0.57233244 -0.092383    0.6169244 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 86\n",
      "Action choisie : 0\n",
      "Observation : [ 0.10456744 -0.76605064 -0.08004451  0.8791406 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 87\n",
      "Action choisie : 0\n",
      "Observation : [ 0.08924642 -0.9599991  -0.0624617   1.1456238 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 88\n",
      "Action choisie : 1\n",
      "Observation : [ 0.07004644 -0.76411945 -0.03954922  0.83402556]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 89\n",
      "Action choisie : 0\n",
      "Observation : [ 0.05476405 -0.9586794  -0.02286871  1.1140128 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 90\n",
      "Action choisie : 0\n",
      "Observation : [ 3.5590466e-02 -1.1534938e+00 -5.8845477e-04  1.3994350e+00]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 91\n",
      "Action choisie : 1\n",
      "Observation : [ 0.01252059 -0.9583644   0.02740025  1.1065682 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 92\n",
      "Action choisie : 1\n",
      "Observation : [-0.0066467  -0.7636132   0.04953161  0.8226057 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 93\n",
      "Action choisie : 0\n",
      "Observation : [-0.02191896 -0.9593766   0.06598373  1.1304469 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 94\n",
      "Action choisie : 1\n",
      "Observation : [-0.04110649 -0.7651777   0.08859266  0.85916775]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 95\n",
      "Action choisie : 0\n",
      "Observation : [-0.05641005 -0.9613875   0.10577602  1.1783403 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 96\n",
      "Action choisie : 0\n",
      "Observation : [-0.0756378  -1.157712    0.12934282  1.5022213 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 97\n",
      "Action choisie : 1\n",
      "Observation : [-0.09879204 -0.9643758   0.15938725  1.2525603 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 98\n",
      "Action choisie : 0\n",
      "Observation : [-0.11807956 -1.1611395   0.18443845  1.5906265 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n",
      "Essai 99\n",
      "Action choisie : 0\n",
      "Observation : [-0.14130235 -1.35791     0.21625099  1.9346943 ]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : True\n",
      "----------------------------------------\n",
      "L’épisode est terminé. Réinitialisation de l’environnement.\n",
      "\n",
      "Essai 100\n",
      "Action choisie : 0\n",
      "Observation : [ 0.03061423 -0.1461086   0.00440608  0.25003394]\n",
      "Récompense : 1.0\n",
      "Épisode terminé ? : False\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Création de l'environnement\n",
    "env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Faire plusieurs essais\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()  # Action aléatoire\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    print(f\"Essai {i + 1}\")\n",
    "    print(f\"Action choisie : {action}\")\n",
    "    print(f\"Observation : {observation}\")\n",
    "    print(f\"Récompense : {reward}\")\n",
    "    print(f\"Épisode terminé ? : {done}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    if done:\n",
    "        print(\"L’épisode est terminé. Réinitialisation de l’environnement.\\n\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Contrôle Manuel de l'Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif: Permettre à l'utilisateur de contrôler manuellement l'agent pour mieux comprendre l'effet des actions.\n",
    "\n",
    "✔Instructions:\n",
    "\n",
    "1. Demander à l'utilisateur d'entrer une action (o ou 1).\n",
    "2. Exécuter l'action dans l'environnement et afficher les nouvelles observations.\n",
    "3. Répéter l'opération jusqu'à la fin de l'épisode.\n",
    "4. Afficher la durée totale de l'épisode avant qu'il ne se termine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Contrôle manuel : entre 0 (gauche) ou 1 (droite)\n",
      "\n",
      " Observation actuelle : [-0.03935112 -0.01441945  0.00508797 -0.04273007]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [-0.03963951 -0.20961398  0.00423337  0.2515538 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.03963951 -0.20961398  0.00423337  0.2515538 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [-0.04383179 -0.40479612  0.00926445  0.545569  ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.04383179 -0.40479612  0.00926445  0.545569  ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [-0.05192772 -0.60004705  0.02017583  0.8411565 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.05192772 -0.60004705  0.02017583  0.8411565 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [-0.06392866 -0.7954385   0.03699896  1.1401154 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.06392866 -0.7954385   0.03699896  1.1401154 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [-0.07983743 -0.9910241   0.05980127  1.4441682 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.07983743 -0.9910241   0.05980127  1.4441682 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.09965791 -0.79668695  0.08868463  1.1707546 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.09965791 -0.79668695  0.08868463  1.1707546 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.11559165 -0.602823    0.11209972  0.9071407 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.11559165 -0.602823    0.11209972  0.9071407 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.1276481  -0.4093826   0.13024254  0.6516896 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.1276481  -0.4093826   0.13024254  0.6516896 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.13583577 -0.216292    0.14327632  0.4026899 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.13583577 -0.216292    0.14327632  0.4026899 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.1401616  -0.0234619   0.15133013  0.15838778]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.1401616  -0.0234619   0.15133013  0.15838778]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.14063084  0.16920604  0.15449788 -0.08298981]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.14063084  0.16920604  0.15449788 -0.08298981]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.13724671  0.36181465  0.15283808 -0.32321957]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.13724671  0.36181465  0.15283808 -0.32321957]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.13001043  0.55446744  0.14637369 -0.56407046]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.13001043  0.55446744  0.14637369 -0.56407046]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.11892107  0.7472652   0.13509229 -0.80729425]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.11892107  0.7472652   0.13509229 -0.80729425]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.10397577  0.94030267  0.1189464  -1.0546157 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.10397577  0.94030267  0.1189464  -1.0546157 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.08516972  1.1336644   0.09785409 -1.307721  ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.08516972  1.1336644   0.09785409 -1.307721  ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.06249643  1.3274195   0.07169966 -1.5682403 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.06249643  1.3274195   0.07169966 -1.5682403 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [-0.03594804  1.1315182   0.04033485 -1.2540817 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.03594804  1.1315182   0.04033485 -1.2540817 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [-0.01331767  1.3261011   0.01525322 -1.5338634 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [-0.01331767  1.3261011   0.01525322 -1.5338634 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [ 0.01320435  1.1307988  -0.01542405 -1.2364597 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [ 0.01320435  1.1307988  -0.01542405 -1.2364597 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [ 0.03582032  1.3261155  -0.04015324 -1.5339345 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [ 0.03582032  1.3261155  -0.04015324 -1.5339345 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [ 0.06234263  1.1314995  -0.07083193 -1.2540478 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [ 0.06234263  1.1314995  -0.07083193 -1.2540478 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [ 0.08497262  1.3274536  -0.09591289 -1.5680488 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [ 0.08497262  1.3274536  -0.09591289 -1.5680488 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [ 0.11152169  1.133599   -0.12727386 -1.3067588 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [ 0.11152169  1.133599   -0.12727386 -1.3067588 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [ 0.13419367  1.3300836  -0.15340903 -1.6364193 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [ 0.13419367  1.3300836  -0.15340903 -1.6364193 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 0\n",
      " Nouvelle observation : [ 0.16079535  1.1370579  -0.18613742 -1.3952067 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " Observation actuelle : [ 0.16079535  1.1370579  -0.18613742 -1.3952067 ]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tape ton action (0=gauche, 1=droite) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action effectuée : 1\n",
      " Nouvelle observation : [ 0.1835365   1.3339432  -0.21404156 -1.7398423 ]\n",
      " Récompense reçue : 1.0\n",
      "\n",
      " L’épisode est terminé.\n",
      " Durée totale de l’épisode : 27 étapes.\n"
     ]
    }
   ],
   "source": [
    "# Création de l’environnement\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "step_count = 0  # Durée de l’épisode (en nombre d’actions)\n",
    "\n",
    "print(\" Contrôle manuel : entre 0 (gauche) ou 1 (droite)\")\n",
    "\n",
    "while not done:\n",
    "    # Afficher l’état courant\n",
    "    print(f\"\\n Observation actuelle : {observation}\")\n",
    "\n",
    "    # Demander l'action à l'utilisateur\n",
    "    user_input = input(\"Tape ton action (0=gauche, 1=droite) : \")\n",
    "\n",
    "    # Vérification de l’entrée\n",
    "    if user_input not in [\"0\", \"1\"]:\n",
    "        print(\" Entrée invalide. Merci de taper uniquement 0 ou 1.\")\n",
    "        continue\n",
    "\n",
    "    action = int(user_input)\n",
    "\n",
    "    # Appliquer l’action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    step_count += 1\n",
    "\n",
    "    # Afficher les infos après action\n",
    "    print(f\" Action effectuée : {action}\")\n",
    "    print(f\" Nouvelle observation : {observation}\")\n",
    "    print(f\" Récompense reçue : {reward}\")\n",
    "\n",
    "# Fin de l’épisode\n",
    "print(\"\\n L’épisode est terminé.\")\n",
    "print(f\" Durée totale de l’épisode : {step_count} étapes.\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: Évaluation des Performances d'une Politique Aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif: Mesurer la durée moyenne d'un épisode lorsqu'un agent prend des actions aléatoires.\n",
    "\n",
    "\n",
    "✔Instructions:\n",
    "\n",
    "1. Faire exécuter à l'agent des actions aléatoires pendant plusieurs épisodes (ex: 10 épisodes).\n",
    "2. Calculer la durée moyenne avant que l'épisode ne se termine.\n",
    "3. Comparer les résultats entre plusieurs exécutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Épisode 1 terminé après 17 étapes.\n",
      " Épisode 2 terminé après 10 étapes.\n",
      " Épisode 3 terminé après 15 étapes.\n",
      " Épisode 4 terminé après 14 étapes.\n",
      " Épisode 5 terminé après 29 étapes.\n",
      " Épisode 6 terminé après 14 étapes.\n",
      " Épisode 7 terminé après 24 étapes.\n",
      " Épisode 8 terminé après 22 étapes.\n",
      " Épisode 9 terminé après 8 étapes.\n",
      " Épisode 10 terminé après 14 étapes.\n",
      "\n",
      " Résultats sur 10 épisodes aléatoires :\n",
      "Durées des épisodes : [17, 10, 15, 14, 29, 14, 24, 22, 8, 14]\n",
      "Durée moyenne : 16.70 étapes\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "num_episodes = 10\n",
    "durations = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Action aléatoire\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        step_count += 1\n",
    "\n",
    "    durations.append(step_count)\n",
    "    print(f\" Épisode {episode + 1} terminé après {step_count} étapes.\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Moyenne\n",
    "average_duration = sum(durations) / num_episodes\n",
    "print(\"\\n Résultats sur 10 épisodes aléatoires :\")\n",
    "print(f\"Durées des épisodes : {durations}\")\n",
    "print(f\"Durée moyenne : {average_duration:.2f} étapes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
