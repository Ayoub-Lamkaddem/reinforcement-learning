{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING II â€“ TÐ  2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif:\n",
    "\n",
    "L'objectif de ce TP est de mettre en pratique les concepts fondamentaux de l'apprentissage parrenforcement en explorant l'algorithmeQ-Learning. Ã€ travers une sÃ©rie d'exercices progressifs.\n",
    "les Ã©tudiants vont apprendre Ã  implÃ©menter cet algorithme, comprendre l'impact des stratÃ©gies d'exploration et d'exploitation, et analyser la convergence des valeurs Q. L'environnement FrozenLak de OpenAl Gym servira de terrain d'expÃ©rimentation, permettant d'illustrer concrÃ¨tement comment un agent apprend Ã  optimiser ses dÃ©cisions grÃ¢ce aux mises Ã  jour successives de sa Q-table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Exploration de l'environnement FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions : \n",
    "1. charger l'envirenement FrozenLake-v1 de OpenAI Gym\n",
    "2. Afficher les informations de l'espace d'etats et d'actions.\n",
    "3. Execute une boucle oÃ¹ l'agent prend des actions aleatoires pendant plusieurs episodes.\n",
    "4. Observer les obeservations et les recompenses obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Espace dâ€™Ã©tats : Discrete(16)\n",
      " Espace dâ€™actions : Discrete(4)\n",
      "\n",
      " Ã‰pisode 1\n",
      "Ã‰tape 1: Ã©tat=0, action=2 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=4, action=2 -> nouvel Ã©tat=8, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=8, action=0 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=4, action=0 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 5: Ã©tat=4, action=0 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 6: Ã©tat=4, action=3 -> nouvel Ã©tat=5, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 2\n",
      "Ã‰tape 1: Ã©tat=0, action=2 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=4, action=0 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=4, action=2 -> nouvel Ã©tat=8, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=8, action=1 -> nouvel Ã©tat=12, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 3\n",
      "Ã‰tape 1: Ã©tat=0, action=2 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=0, action=1 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=1, action=1 -> nouvel Ã©tat=5, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 4\n",
      "Ã‰tape 1: Ã©tat=0, action=3 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=1, action=0 -> nouvel Ã©tat=5, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 5\n",
      "Ã‰tape 1: Ã©tat=0, action=1 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=1, action=1 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=0, action=0 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=4, action=1 -> nouvel Ã©tat=8, rÃ©compense=0.0\n",
      "Ã‰tape 5: Ã©tat=8, action=1 -> nouvel Ã©tat=8, rÃ©compense=0.0\n",
      "Ã‰tape 6: Ã©tat=8, action=2 -> nouvel Ã©tat=9, rÃ©compense=0.0\n",
      "Ã‰tape 7: Ã©tat=9, action=0 -> nouvel Ã©tat=5, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 6\n",
      "Ã‰tape 1: Ã©tat=0, action=1 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=4, action=1 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=4, action=3 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=0, action=2 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 5: Ã©tat=0, action=2 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 6: Ã©tat=0, action=2 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 7: Ã©tat=4, action=3 -> nouvel Ã©tat=5, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 7\n",
      "Ã‰tape 1: Ã©tat=0, action=1 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=4, action=3 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=0, action=2 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=4, action=1 -> nouvel Ã©tat=5, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 8\n",
      "Ã‰tape 1: Ã©tat=0, action=3 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=0, action=1 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=0, action=2 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=1, action=0 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 5: Ã©tat=1, action=1 -> nouvel Ã©tat=5, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 9\n",
      "Ã‰tape 1: Ã©tat=0, action=3 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=1, action=1 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=0, action=1 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=0, action=0 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 5: Ã©tat=0, action=3 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 6: Ã©tat=0, action=3 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 7: Ã©tat=1, action=3 -> nouvel Ã©tat=2, rÃ©compense=0.0\n",
      "Ã‰tape 8: Ã©tat=2, action=2 -> nouvel Ã©tat=2, rÃ©compense=0.0\n",
      "Ã‰tape 9: Ã©tat=2, action=3 -> nouvel Ã©tat=2, rÃ©compense=0.0\n",
      "Ã‰tape 10: Ã©tat=2, action=2 -> nouvel Ã©tat=2, rÃ©compense=0.0\n",
      "Ã‰tape 11: Ã©tat=2, action=2 -> nouvel Ã©tat=2, rÃ©compense=0.0\n",
      "Ã‰tape 12: Ã©tat=2, action=2 -> nouvel Ã©tat=6, rÃ©compense=0.0\n",
      "Ã‰tape 13: Ã©tat=6, action=3 -> nouvel Ã©tat=7, rÃ©compense=0.0\n",
      "\n",
      " Ã‰pisode 10\n",
      "Ã‰tape 1: Ã©tat=0, action=0 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 2: Ã©tat=0, action=3 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 3: Ã©tat=0, action=1 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 4: Ã©tat=0, action=1 -> nouvel Ã©tat=4, rÃ©compense=0.0\n",
      "Ã‰tape 5: Ã©tat=4, action=0 -> nouvel Ã©tat=0, rÃ©compense=0.0\n",
      "Ã‰tape 6: Ã©tat=0, action=1 -> nouvel Ã©tat=1, rÃ©compense=0.0\n",
      "Ã‰tape 7: Ã©tat=1, action=0 -> nouvel Ã©tat=5, rÃ©compense=0.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Charger lâ€™environnement FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Afficher lâ€™espace dâ€™Ã©tats et dâ€™actions\n",
    "print(\" Espace dâ€™Ã©tats :\", env.observation_space)\n",
    "print(\" Espace dâ€™actions :\", env.action_space)\n",
    "\n",
    "num_episodes = 10  # Tu peux augmenter si tu veux\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(f\"\\n Ã‰pisode {episode + 1}\")\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Action alÃ©atoire\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "\n",
    "        print(f\"Ã‰tape {step}: Ã©tat={observation}, action={action} -> nouvel Ã©tat={new_observation}, rÃ©compense={reward}\")\n",
    "        observation = new_observation\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: ImplÃ©menintion de la Q-Table et Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "1. CrÃ©er une Q-Table de dimension (nombre d'Ã©tats x nombre d'actions),\n",
    "initialisÃ©e a 0.\n",
    "2. Afficher la Q-Table avant l'apprentissage.\n",
    "3. VÃ©rifier que chaque Ã©tat a une liste de valeurs associÃ©es aux actions possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre dâ€™Ã©tats : 16\n",
      "Nombre dâ€™actions : 4\n",
      "\n",
      " Q-Table initialisÃ©e\n",
      "\n",
      " VÃ©rification des valeurs Q par Ã©tat :\n",
      "Ã‰tat 0 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 1 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 2 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 3 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 4 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 5 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 6 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 7 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 8 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 9 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 10 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 11 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 12 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 13 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 14 â†’ Valeurs Q : [0. 0. 0. 0.]\n",
      "Ã‰tat 15 â†’ Valeurs Q : [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Charger lâ€™environnement\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# RÃ©cupÃ©rer le nombre d'Ã©tats et d'actions\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Initialisation de la Q-table Ã  0\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Affichage des dimensions\n",
    "print(f\"Nombre dâ€™Ã©tats : {num_states}\")\n",
    "print(f\"Nombre dâ€™actions : {num_actions}\")\n",
    "\n",
    "# Affichage de la Q-table initiale\n",
    "print(\"\\n Q-Table initialisÃ©e\")\n",
    "print(\"\\n VÃ©rification des valeurs Q par Ã©tat :\")\n",
    "for state in range(num_states):\n",
    "    print(f\"Ã‰tat {state} â†’ Valeurs Q : {q_table[state]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3: ImplÃ©mentation du Q-Learning avec Mise Ã  Jour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions :\n",
    "\n",
    "1. DÃ©finir les hyperparamÃ¨tres: taux d'apprentissage (alpha), facteur de discount (gamma), epsilon pour lexploration.\n",
    "2. Mettre Ã  jour la Q-Table en appliquant la rÃ¨gle de mise Ã  jour du Q-Learning:\n",
    "Q(3, a) = Q(s, a) +a [R+ ymax Q(e', 2') -Q(s, a)]\n",
    "3. ExÃ©cuter plusieurs Ã©pisodes et observer l'Ã©volution de la table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Q-Table aprÃ¨s apprentissage :\n",
      "[[0.49680779 0.45976383 0.43714378 0.4490385 ]\n",
      " [0.08699297 0.02049842 0.0212858  0.32892152]\n",
      " [0.07102829 0.07773071 0.11881115 0.17172254]\n",
      " [0.01311517 0.04665465 0.02827669 0.02697972]\n",
      " [0.50728623 0.37233412 0.33899513 0.35983718]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20699255 0.04985315 0.07987513 0.03405345]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35007753 0.40365046 0.49283163 0.54538629]\n",
      " [0.37295882 0.59216091 0.49351665 0.42129451]\n",
      " [0.49553654 0.32005325 0.26203592 0.18399582]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47862796 0.49751853 0.72498161 0.34139484]\n",
      " [0.61093637 0.79019074 0.76025221 0.63718633]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# CrÃ©ation de lâ€™environnement\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Q-table initialisÃ©e Ã  0\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# HyperparamÃ¨tres\n",
    "alpha = 0.1            # Taux dâ€™apprentissage\n",
    "gamma = 0.99           # Facteur de discount\n",
    "epsilon = 1.0          # Niveau dâ€™exploration initial\n",
    "epsilon_decay = 0.995  # DÃ©croissance dâ€™epsilon\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 5000    # Nombre dâ€™Ã©pisodes dâ€™apprentissage\n",
    "\n",
    "# Apprentissage\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choisir action (Îµ-greedy)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Action alÃ©atoire\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Action selon la Q-table\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Mise Ã  jour de la Q-table (formule Q-learning)\n",
    "        best_next_action = np.max(q_table[next_state])\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (\n",
    "            reward + gamma * best_next_action - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Mise Ã  jour de epsilon (exploration diminue)\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Affichage de la Q-table finale\n",
    "print(\"\\n Q-Table aprÃ¨s apprentissage :\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices 4: Ã‰valuation des Performances de l'Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iustructions:\n",
    "\n",
    "\n",
    "1. Lancer 100 Ã©pisodes en utilisant uniquement l'action optimale (argmax(O[s, Ð°])). \n",
    "2. Mesurer le taux de rÃ©ussite de l'agent (nombre de fois oÃ¹ il atteint l'objectii). \n",
    "3. Comparer les performances avec celles obtenues dans l'Exercice ! (actions alÃ©atoires)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Lâ€™agent a rÃ©ussi Ã  atteindre lâ€™objectif 69 fois sur 100.\n",
      "ðŸŽ¯ Taux de rÃ©ussite : 69.00 %\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Action optimale selon la Q-table\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "\n",
    "        if done and reward == 1.0:\n",
    "            successes += 1\n",
    "\n",
    "# Taux de rÃ©ussite\n",
    "success_rate = (successes / num_test_episodes) * 100\n",
    "print(f\"\\nâœ… Lâ€™agent a rÃ©ussi Ã  atteindre lâ€™objectif {successes} fois sur {num_test_episodes}.\")\n",
    "print(f\"ðŸŽ¯ Taux de rÃ©ussite : {success_rate:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
