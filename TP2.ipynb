{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING II – TР 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif:\n",
    "\n",
    "L'objectif de ce TP est de mettre en pratique les concepts fondamentaux de l'apprentissage parrenforcement en explorant l'algorithmeQ-Learning. À travers une série d'exercices progressifs.\n",
    "les étudiants vont apprendre à implémenter cet algorithme, comprendre l'impact des stratégies d'exploration et d'exploitation, et analyser la convergence des valeurs Q. L'environnement FrozenLak de OpenAl Gym servira de terrain d'expérimentation, permettant d'illustrer concrètement comment un agent apprend à optimiser ses décisions grâce aux mises à jour successives de sa Q-table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Exploration de l'environnement FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions : \n",
    "1. charger l'envirenement FrozenLake-v1 de OpenAI Gym\n",
    "2. Afficher les informations de l'espace d'etats et d'actions.\n",
    "3. Execute une boucle où l'agent prend des actions aleatoires pendant plusieurs episodes.\n",
    "4. Observer les obeservations et les recompenses obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Espace d’états : Discrete(16)\n",
      " Espace d’actions : Discrete(4)\n",
      "\n",
      " Épisode 1\n",
      "Étape 1: état=0, action=2 -> nouvel état=4, récompense=0.0\n",
      "Étape 2: état=4, action=2 -> nouvel état=8, récompense=0.0\n",
      "Étape 3: état=8, action=0 -> nouvel état=4, récompense=0.0\n",
      "Étape 4: état=4, action=0 -> nouvel état=4, récompense=0.0\n",
      "Étape 5: état=4, action=0 -> nouvel état=4, récompense=0.0\n",
      "Étape 6: état=4, action=3 -> nouvel état=5, récompense=0.0\n",
      "\n",
      " Épisode 2\n",
      "Étape 1: état=0, action=2 -> nouvel état=4, récompense=0.0\n",
      "Étape 2: état=4, action=0 -> nouvel état=4, récompense=0.0\n",
      "Étape 3: état=4, action=2 -> nouvel état=8, récompense=0.0\n",
      "Étape 4: état=8, action=1 -> nouvel état=12, récompense=0.0\n",
      "\n",
      " Épisode 3\n",
      "Étape 1: état=0, action=2 -> nouvel état=0, récompense=0.0\n",
      "Étape 2: état=0, action=1 -> nouvel état=1, récompense=0.0\n",
      "Étape 3: état=1, action=1 -> nouvel état=5, récompense=0.0\n",
      "\n",
      " Épisode 4\n",
      "Étape 1: état=0, action=3 -> nouvel état=1, récompense=0.0\n",
      "Étape 2: état=1, action=0 -> nouvel état=5, récompense=0.0\n",
      "\n",
      " Épisode 5\n",
      "Étape 1: état=0, action=1 -> nouvel état=1, récompense=0.0\n",
      "Étape 2: état=1, action=1 -> nouvel état=0, récompense=0.0\n",
      "Étape 3: état=0, action=0 -> nouvel état=4, récompense=0.0\n",
      "Étape 4: état=4, action=1 -> nouvel état=8, récompense=0.0\n",
      "Étape 5: état=8, action=1 -> nouvel état=8, récompense=0.0\n",
      "Étape 6: état=8, action=2 -> nouvel état=9, récompense=0.0\n",
      "Étape 7: état=9, action=0 -> nouvel état=5, récompense=0.0\n",
      "\n",
      " Épisode 6\n",
      "Étape 1: état=0, action=1 -> nouvel état=4, récompense=0.0\n",
      "Étape 2: état=4, action=1 -> nouvel état=4, récompense=0.0\n",
      "Étape 3: état=4, action=3 -> nouvel état=0, récompense=0.0\n",
      "Étape 4: état=0, action=2 -> nouvel état=0, récompense=0.0\n",
      "Étape 5: état=0, action=2 -> nouvel état=0, récompense=0.0\n",
      "Étape 6: état=0, action=2 -> nouvel état=4, récompense=0.0\n",
      "Étape 7: état=4, action=3 -> nouvel état=5, récompense=0.0\n",
      "\n",
      " Épisode 7\n",
      "Étape 1: état=0, action=1 -> nouvel état=4, récompense=0.0\n",
      "Étape 2: état=4, action=3 -> nouvel état=0, récompense=0.0\n",
      "Étape 3: état=0, action=2 -> nouvel état=4, récompense=0.0\n",
      "Étape 4: état=4, action=1 -> nouvel état=5, récompense=0.0\n",
      "\n",
      " Épisode 8\n",
      "Étape 1: état=0, action=3 -> nouvel état=0, récompense=0.0\n",
      "Étape 2: état=0, action=1 -> nouvel état=0, récompense=0.0\n",
      "Étape 3: état=0, action=2 -> nouvel état=1, récompense=0.0\n",
      "Étape 4: état=1, action=0 -> nouvel état=1, récompense=0.0\n",
      "Étape 5: état=1, action=1 -> nouvel état=5, récompense=0.0\n",
      "\n",
      " Épisode 9\n",
      "Étape 1: état=0, action=3 -> nouvel état=1, récompense=0.0\n",
      "Étape 2: état=1, action=1 -> nouvel état=0, récompense=0.0\n",
      "Étape 3: état=0, action=1 -> nouvel état=0, récompense=0.0\n",
      "Étape 4: état=0, action=0 -> nouvel état=0, récompense=0.0\n",
      "Étape 5: état=0, action=3 -> nouvel état=0, récompense=0.0\n",
      "Étape 6: état=0, action=3 -> nouvel état=1, récompense=0.0\n",
      "Étape 7: état=1, action=3 -> nouvel état=2, récompense=0.0\n",
      "Étape 8: état=2, action=2 -> nouvel état=2, récompense=0.0\n",
      "Étape 9: état=2, action=3 -> nouvel état=2, récompense=0.0\n",
      "Étape 10: état=2, action=2 -> nouvel état=2, récompense=0.0\n",
      "Étape 11: état=2, action=2 -> nouvel état=2, récompense=0.0\n",
      "Étape 12: état=2, action=2 -> nouvel état=6, récompense=0.0\n",
      "Étape 13: état=6, action=3 -> nouvel état=7, récompense=0.0\n",
      "\n",
      " Épisode 10\n",
      "Étape 1: état=0, action=0 -> nouvel état=0, récompense=0.0\n",
      "Étape 2: état=0, action=3 -> nouvel état=0, récompense=0.0\n",
      "Étape 3: état=0, action=1 -> nouvel état=0, récompense=0.0\n",
      "Étape 4: état=0, action=1 -> nouvel état=4, récompense=0.0\n",
      "Étape 5: état=4, action=0 -> nouvel état=0, récompense=0.0\n",
      "Étape 6: état=0, action=1 -> nouvel état=1, récompense=0.0\n",
      "Étape 7: état=1, action=0 -> nouvel état=5, récompense=0.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Charger l’environnement FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Afficher l’espace d’états et d’actions\n",
    "print(\" Espace d’états :\", env.observation_space)\n",
    "print(\" Espace d’actions :\", env.action_space)\n",
    "\n",
    "num_episodes = 10  # Tu peux augmenter si tu veux\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(f\"\\n Épisode {episode + 1}\")\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Action aléatoire\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "\n",
    "        print(f\"Étape {step}: état={observation}, action={action} -> nouvel état={new_observation}, récompense={reward}\")\n",
    "        observation = new_observation\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: Implémenintion de la Q-Table et Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "1. Créer une Q-Table de dimension (nombre d'états x nombre d'actions),\n",
    "initialisée a 0.\n",
    "2. Afficher la Q-Table avant l'apprentissage.\n",
    "3. Vérifier que chaque état a une liste de valeurs associées aux actions possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d’états : 16\n",
      "Nombre d’actions : 4\n",
      "\n",
      " Q-Table initialisée\n",
      "\n",
      " Vérification des valeurs Q par état :\n",
      "État 0 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 1 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 2 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 3 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 4 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 5 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 6 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 7 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 8 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 9 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 10 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 11 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 12 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 13 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 14 → Valeurs Q : [0. 0. 0. 0.]\n",
      "État 15 → Valeurs Q : [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Charger l’environnement\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Récupérer le nombre d'états et d'actions\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Initialisation de la Q-table à 0\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Affichage des dimensions\n",
    "print(f\"Nombre d’états : {num_states}\")\n",
    "print(f\"Nombre d’actions : {num_actions}\")\n",
    "\n",
    "# Affichage de la Q-table initiale\n",
    "print(\"\\n Q-Table initialisée\")\n",
    "print(\"\\n Vérification des valeurs Q par état :\")\n",
    "for state in range(num_states):\n",
    "    print(f\"État {state} → Valeurs Q : {q_table[state]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3: Implémentation du Q-Learning avec Mise à Jour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions :\n",
    "\n",
    "1. Définir les hyperparamètres: taux d'apprentissage (alpha), facteur de discount (gamma), epsilon pour lexploration.\n",
    "2. Mettre à jour la Q-Table en appliquant la règle de mise à jour du Q-Learning:\n",
    "Q(3, a) = Q(s, a) +a [R+ ymax Q(e', 2') -Q(s, a)]\n",
    "3. Exécuter plusieurs épisodes et observer l'évolution de la table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Q-Table après apprentissage :\n",
      "[[0.49680779 0.45976383 0.43714378 0.4490385 ]\n",
      " [0.08699297 0.02049842 0.0212858  0.32892152]\n",
      " [0.07102829 0.07773071 0.11881115 0.17172254]\n",
      " [0.01311517 0.04665465 0.02827669 0.02697972]\n",
      " [0.50728623 0.37233412 0.33899513 0.35983718]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20699255 0.04985315 0.07987513 0.03405345]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35007753 0.40365046 0.49283163 0.54538629]\n",
      " [0.37295882 0.59216091 0.49351665 0.42129451]\n",
      " [0.49553654 0.32005325 0.26203592 0.18399582]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47862796 0.49751853 0.72498161 0.34139484]\n",
      " [0.61093637 0.79019074 0.76025221 0.63718633]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Création de l’environnement\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Q-table initialisée à 0\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Hyperparamètres\n",
    "alpha = 0.1            # Taux d’apprentissage\n",
    "gamma = 0.99           # Facteur de discount\n",
    "epsilon = 1.0          # Niveau d’exploration initial\n",
    "epsilon_decay = 0.995  # Décroissance d’epsilon\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 5000    # Nombre d’épisodes d’apprentissage\n",
    "\n",
    "# Apprentissage\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choisir action (ε-greedy)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Action aléatoire\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Action selon la Q-table\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Mise à jour de la Q-table (formule Q-learning)\n",
    "        best_next_action = np.max(q_table[next_state])\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (\n",
    "            reward + gamma * best_next_action - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Mise à jour de epsilon (exploration diminue)\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Affichage de la Q-table finale\n",
    "print(\"\\n Q-Table après apprentissage :\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices 4: Évaluation des Performances de l'Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iustructions:\n",
    "\n",
    "\n",
    "1. Lancer 100 épisodes en utilisant uniquement l'action optimale (argmax(O[s, а])). \n",
    "2. Mesurer le taux de réussite de l'agent (nombre de fois où il atteint l'objectii). \n",
    "3. Comparer les performances avec celles obtenues dans l'Exercice ! (actions aléatoires)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ L’agent a réussi à atteindre l’objectif 69 fois sur 100.\n",
      "🎯 Taux de réussite : 69.00 %\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Action optimale selon la Q-table\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "\n",
    "        if done and reward == 1.0:\n",
    "            successes += 1\n",
    "\n",
    "# Taux de réussite\n",
    "success_rate = (successes / num_test_episodes) * 100\n",
    "print(f\"\\n✅ L’agent a réussi à atteindre l’objectif {successes} fois sur {num_test_episodes}.\")\n",
    "print(f\"🎯 Taux de réussite : {success_rate:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
